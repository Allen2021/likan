---
title: "Visual World Paradigm"
subtitle: "An eye-tracking technique"
author: "Likan Zhan"
date: '2018-02-07'
lastmod: "2018-02-19"
bibliography: [../bibliography/paradigm.bib]
csl: [../bibliography/apa-old-doi-prefix.csl]
output:
  blogdown::html_page:
    toc: true
slug: []
tags: []
categories: []
---


<div id="TOC">
<ul>
<li><a href="#a-brief-description.">1. A brief description.</a></li>
<li><a href="#recent-applications">2. Recent Applications</a></li>
<li><a href="#references">3. References</a></li>
</ul>
</div>

<div id="a-brief-description." class="section level1">
<h1>1. A brief description.</h1>
<p>This paradigm relies on two seminal work published by <span class="citation">Cooper (1974)</span> and by <span class="citation">Tanenhaus, Spivey-Knowlton, Eberhard, &amp; Sedivy (1995)</span>.</p>
</div>
<div id="recent-applications" class="section level1">
<h1>2. Recent Applications</h1>
<ol style="list-style-type: decimal">
<li><p><span class="citation">Groot, Huettig, &amp; Olivers (2017)</span></p>
<ul>
<li>On all trials, participants memorized a spoken word for a verbal recognition test at the end of the trial. During the retention period, they performed a visual search task. In crucial trials, the search target were absent. In a crucial trial, for example, the word to remember was “banana”. They then saw four object printed on the screen. These contained an object that was semantically related (such as the monkey), an object that was visually related (such as the canoe), and two objects that were unrelated (such as the hat and the tambourine). In the visual search stage, participant were asked to search the banana (the template condition) or the figurine (Accessory condition). The article observed that participants’ eye movements are significantly different between the accessory condition and the template condition, suggesting that language-induced attentional biases are subject to task requirements.</li>
</ul></li>
</ol>
</div>
<div id="references" class="section level1 unnumbered">
<h1>3. References</h1>
<div id="refs" class="references">
<div id="ref-RN906">
<p>Cooper, R. M. (1974). The control of eye fixation by the meaning of spoken language: A new methodology for the real-time investigation of speech perception, memory, and language processing. <em>Cognitive Psychology</em>, <em>6</em>(1), 84–107. Journal Article. doi:<a href="https://doi.org/10.1016/0010-0285(74)90005-x">10.1016/0010-0285(74)90005-x</a></p>
</div>
<div id="ref-RN913">
<p>Groot, F. de, Huettig, F., &amp; Olivers, C. N. L. (2017). Language-induced visual and semantic biases in visual search are subject to task requirements. <em>Visual Cognition</em>, <em>25</em>(1-3), 225–240. Journal Article. doi:<a href="https://doi.org/10.1080/13506285.2017.1324934">10.1080/13506285.2017.1324934</a></p>
</div>
<div id="ref-RN907">
<p>Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K. M., &amp; Sedivy, J. C. (1995). Integration of visual and linguistic information in spoken language comprehension. <em>Science</em>, <em>268</em>(5217), 1632–1634. Journal Article. doi:<a href="https://doi.org/10.1126/science.7777863">10.1126/science.7777863</a></p>
</div>
</div>
</div>
