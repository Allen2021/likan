---
title: "Visual World Paradigm"
subtitle: "An eye-tracking technique"
author: "Likan Zhan"
date: '2018-02-07'
lastmod: "2018-03-18"
bibliography: [../bibliography/paradigm.bib]
link-citations: yes
csl: [../bibliography/apa-old-doi-prefix.csl]
output:
  blogdown::html_page:
    toc: true
slug: []
tags: []
categories: []
---


<div id="TOC">
<ul>
<li><a href="#a-brief-description">1. A brief description</a></li>
<li><a href="#recent-applications">2. Recent Applications</a></li>
<li><a href="#references">3. References</a></li>
</ul>
</div>

<div id="a-brief-description" class="section level1">
<h1>1. A brief description</h1>
<p>This paradigm relies on two seminal work published by <span class="citation">Cooper (<a href="#ref-RN906">1974</a>)</span> and by <span class="citation">Tanenhaus, Spivey-Knowlton, Eberhard, &amp; Sedivy (<a href="#ref-RN907">1995</a>)</span>.</p>
</div>
<div id="recent-applications" class="section level1">
<h1>2. Recent Applications</h1>
<ol style="list-style-type: decimal">
<li><span class="citation">Groot, Huettig, &amp; Olivers (<a href="#ref-RN913">2017</a>)</span></li>
</ol>
<p>On all trials, participants memorized a spoken word for a verbal recognition test at the end of the trial. During the retention period, they performed a visual search task. In crucial trials, the search target were absent. In a crucial trial, for example, the word to remember was “banana”. They then saw four object printed on the screen. These contained an object that was semantically related (such as the monkey), an object that was visually related (such as the canoe), and two objects that were unrelated (such as the hat and the tambourine). In the visual search stage, participant were asked to search the banana (the template condition) or the figurine (Accessory condition). The article observed that participants’ eye movements are significantly different between the accessory condition and the template condition, suggesting that language-induced attentional biases are subject to task requirements.</p>
<ol start="2" style="list-style-type: decimal">
<li><span class="citation">Saryazdi &amp; Chambers (<a href="#ref-RN931">2018</a>)</span></li>
</ol>
<p>To explore the effects of the degree of image realism, researchers conducted two eye tracking studies using the visual world paradigm. The test image consist of four objects, such as a cigarette, a banana, an earings, and an apple. The test images consist of both the phorographs and the clipart images of the same objects. The two experiments differ in whether the test audios are noun-biased (Experiment 1), such as <em>John will move the apple/banana</em>, or verb-biased (Experiment 2), such as <em>John will move/peal the apple/banana</em>. Researchers found a modest benefit for clipart stimuli during real-time processing, but only for noun-driving mappings, i.e., the effect of realism was observed in experiment 1 but not in experiment 2.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>3. References</h1>
<div id="refs" class="references">
<div id="ref-RN906">
<p>Cooper, R. M. (1974). The control of eye fixation by the meaning of spoken language: A new methodology for the real-time investigation of speech perception, memory, and language processing. <em>Cognitive Psychology</em>, <em>6</em>(1), 84–107. Journal Article. doi:<a href="https://doi.org/10.1016/0010-0285(74)90005-x">10.1016/0010-0285(74)90005-x</a></p>
</div>
<div id="ref-RN913">
<p>Groot, F. de, Huettig, F., &amp; Olivers, C. N. L. (2017). Language-induced visual and semantic biases in visual search are subject to task requirements. <em>Visual Cognition</em>, <em>25</em>(1-3), 225–240. Journal Article. doi:<a href="https://doi.org/10.1080/13506285.2017.1324934">10.1080/13506285.2017.1324934</a></p>
</div>
<div id="ref-RN931">
<p>Saryazdi, R., &amp; Chambers, C. G. (2018). Mapping language to visual referents: Does the degree of image realism matter? <em>Acta Psychologica</em>, <em>182</em>, 91–99. Journal Article. doi:<a href="https://doi.org/10.1016/j.actpsy.2017.11.003">10.1016/j.actpsy.2017.11.003</a></p>
</div>
<div id="ref-RN907">
<p>Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K. M., &amp; Sedivy, J. C. (1995). Integration of visual and linguistic information in spoken language comprehension. <em>Science</em>, <em>268</em>(5217), 1632–1634. Journal Article. doi:<a href="https://doi.org/10.1126/science.7777863">10.1126/science.7777863</a></p>
</div>
</div>
</div>
